{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Dimension Reduction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 為容易理解高維資料的分佈情況及降低後續特徵提取演算量，最常用的方式就是將資料「降維(Dimensionality Reduction)」\n",
    "到二維或三維空間再進行觀察，亦可看做是將資料從高維度重新投影(Projection)至低維度空間，包括\n",
    "    * 線性 <br>\n",
    "        A. 主成份分析(Principle Component Analysis, PCA) <br>\n",
    "        線性區別分析(Linear Discriminant Analysis, LDA) <br>\n",
    "        B. 非負矩陣分解(Non-negative Matrix Factorization, NMF) <br>\n",
    "    * 非線性<br>\n",
    "        C. t-分佈隨機鄰近嵌入(t-distributed Stochastic Neighbor Embedding, t-SNE) <br>\n",
    "        D. 均勻流形逼近及投影(Uniform Manifold Approximation and Projection, UMAP) <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid green\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. 主成分析 (Principal Component Analysis, PCA)\n",
    "PCA is fundamentally a dimensionality reduction algorithm, but it can also be useful as a tool for visualization, for noise filtering, for feature extraction and engineering, and much more.\n",
    "https://www.youtube.com/watch?v=g-Hb26agBFg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://miro.medium.com/v2/resize:fit:1100/format:webp/0*S4yAgwWKpy0ud5-q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"PCA.bin\" width=\"800\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 主成分分析步驟\n",
    "    * Step 1: Find mean and normalization (標準化d維原”數據集”)。\n",
    "    * Step 2: Derive the covariance matrix (建立共變異數矩陣)。\n",
    "    * Step 3: Find the eigenvalues and eigenvectors (特徵值與特徵向量)。\n",
    "    * Step 4: Select principle componments (選取k個最大特徵值相對應k個的特徵向量，其中k即為新特徵子空間的維數)。\n",
    "    * Step 5: Transpose to dimension reduced data (使用排序最上面的k個的特徵向量，建立投影矩陣W以轉換原本d維的原數據至新的k維特徵子空間)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA Implemenration\n",
    "\n",
    "http://localhost:8888/notebooks/Machine%20Learning%20with%20Application/3.%20Unsupervised%20Learning/PCA_Implementatoin.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()\n",
    "\n",
    "fig, axes = plt.subplots(2, 5, figsize=(8, 5),\n",
    "                         subplot_kw={'xticks':(), 'yticks': ()})\n",
    "for ax, img in zip(axes.ravel(), digits.images):\n",
    "    ax.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(digits.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "# build a PCA model\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(digits.data)\n",
    "\n",
    "# transform the digits data onto the first two principal components\n",
    "digits_pca = pca.transform(digits.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [\"#476A2A\", \"#7851B8\", \"#BD3430\", \"#4A2D4E\", \"#875525\",\n",
    "          \"#A83683\", \"#4E655E\", \"#853541\", \"#3A3120\", \"#535D8E\"]\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.xlim(digits_pca[:, 0].min(), digits_pca[:, 0].max())\n",
    "plt.ylim(digits_pca[:, 1].min(), digits_pca[:, 1].max())\n",
    "\n",
    "for i in range(len(digits.data)):\n",
    "    # actually plot the digits as text instead of using scatter\n",
    "    plt.text(digits_pca[i, 0], digits_pca[i, 1], str(digits.target[i]),\n",
    "             color = colors[digits.target[i]],\n",
    "             fontdict={'weight': 'bold', 'size': 9})\n",
    "plt.xlabel(\"First principal component\")\n",
    "plt.ylabel(\"Second principal component\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Application: Eigenfaces for feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_lfw_people\n",
    "people = fetch_lfw_people(min_faces_per_person=20, resize=0.7)\n",
    " # min_faces_per_person: The extracted dataset will only retain pictures of people that have at least min_faces_per_person different pictures\n",
    "image_shape = people.images[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"people.images.shape: {people.images.shape}\")\n",
    "print(f\"Number of classes: {len(people.target_names)}\")\n",
    "print(f'1st people name: {people.target_names[people.target[1]]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 5, figsize=(8, 5),\n",
    "                         subplot_kw={'xticks': (), 'yticks': ()})\n",
    "\n",
    "for target, image, ax in zip(people.target, people.images, axes.ravel()):\n",
    "    ax.imshow(image)\n",
    "    ax.set_title(people.target_names[target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count how often each target appears\n",
    "counts = np.bincount(people.target)\n",
    "# print counts next to target names:\n",
    "for i, (count, name) in enumerate(zip(counts, people.target_names)):\n",
    "    print(\"{0:25} {1:3}\".format(name, count), end='   ')\n",
    "    if (i + 1) % 3 == 0:\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(people.target) # take the unique index of the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.zeros(people.target.shape, dtype = np.bool_)\n",
    "len(mask)  # sample sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(people.target == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for target in np.unique(people.target):\n",
    "    mask[np.where(people.target == target)[0][:50]] = 1\n",
    "    \n",
    "X_people = people.data[mask]\n",
    "y_people = people.target[mask]\n",
    "\n",
    "# scale the grey-scale values to be between 0 and 1\n",
    "# instead of 0 and 255 for better numeric stability:\n",
    "X_people = X_people / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "# split the data in training and test set\n",
    "people_X_train, people_X_test, people_y_train, people_y_test = train_test_split(\n",
    "    X_people, y_people, stratify=y_people, random_state=0)\n",
    "# build a KNeighborsClassifier with using one neighbor:\n",
    "people_knn = KNeighborsClassifier(n_neighbors=50)\n",
    "people_knn.fit(people_X_train, people_y_train)\n",
    "print(\"Test set score of 1-nn: {:.2f}\".format(people_knn.score(people_X_test, people_y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people_pca = PCA(n_components=100, whiten=True, random_state=0).fit(people_X_train)\n",
    "people_X_train_pca = people_pca.transform(people_X_train)\n",
    "people_X_test_pca = people_pca.transform(people_X_test)\n",
    "\n",
    "print(\"X_train_pca.shape: {}\".format(people_X_train_pca.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"pca.components_.shape: {}\".format(people_pca.components_.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 5, figsize=(8, 5),\n",
    "                         subplot_kw={'xticks': (), 'yticks': ()})\n",
    "for i, (component, ax) in enumerate(zip(people_pca.components_, axes.ravel())):\n",
    "    ax.imshow(component.reshape(image_shape),\n",
    "              cmap='viridis')\n",
    "    ax.set_title(\"{}. component\".format((i + 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由於PCA主要目的是找出資料集中最重要的特徵維度及使其有最大分佈，並不關心資料原始排列結構及相互關連性，所以當資料維度變大時，只靠一個或兩個軸向的表示就變得難以正確的進行資料分類。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* PCA is widely used for dimensionality reduction and is a linear transformation in the sense that the new dimensions it produces are a linear combination of the original dimensions. \n",
    "* It focusses more on the dissimilar points because it tries to maximize the distance between the points that are far away in high dimensions.\n",
    "* However, this comes at the cost of assuming a linear global structure of the dataset. \n",
    "* So, for non-linear datasets where large distances are usually less indicative of the actual structure of the dataset (check the image below), it makes more sense to use small distances between points, i.e. consider neighboring points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LDA是PCA延伸的一種方法，\n",
    "- PCA目標是希望找到投影軸讓資料投影下去後分散量最大化，但PCA不需要知道資料的類別。\n",
    "- LDA也是希望資料投影下去後分散量最大，但不同的是這個分散量是希望「不同類別之間的分散量」越大越好。\n",
    "- LDA和PCA差異: PCA是無監督式(unsupervised learning)方法，LDA多了這四個字(「不同類別」)是一種監督式(supervised learning)方法\n",
    "    \n",
    "https://chih-sheng-huang821.medium.com/%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92-%E9%99%8D%E7%B6%AD-dimension-reduction-%E7%B7%9A%E6%80%A7%E5%8D%80%E5%88%A5%E5%88%86%E6%9E%90-linear-discriminant-analysis-d4c40c4cf937"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid green\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. 非負矩陣分解 (Non-Negative Matrix Factorization, NMF)\n",
    "\n",
    "The why and how of nonnegative matrix factorization: https://blog.acolyer.org/2019/02/18/the-why-and-how-of-nonnegative-matrix-factorization/ <br>\n",
    "NMF was first introduced by Paatero and Tapper in 1994 and popularised in an article by Lee and Seung in 1999. Since then, the number of publications referencing the technique has grown rapidly.\n",
    "\n",
    "* MF approximates a matrix $\\mathbf{X}$ with a low-rank matrix approximation such that $\\mathbf{X}\\approx \\mathbf{WH}$.\n",
    "* NMF has become so popular because of its ability to extract sparse and easily interpretable factors automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"https://blog.acolyer.org/wp-content/uploads/2019/02/nmf-fig-1.jpeg?w=640\" width=\"1000\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume there exists a dataset $\\mathbf{X}$, we can seperate $\\mathbf{X} $ into the dictionary of elementary recurring features of the data $\\mathbf{W}$ and the activations of the different features $\\mathbf{H}$\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "    \\mathbf{X} \\approx \\mathbf{WH}, \\mathbf{W} \\geq 0 , \\mathbf{H} \\geq 0\n",
    "\\end{split}. \\tag{B.1}\n",
    "\\end{equation} \n",
    "\n",
    "where $k$ is the number of latent features, such as $\\mathbf{X} \\in \\boldsymbol{R}^{p \\times n}; \\mathbf{W} \\in \\boldsymbol{R}^{p \\times r}; \\mathbf{H} \\in \\boldsymbol{R}^{r \\times n}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 1: Initialize the basis matrix $\\mathbf{H}$ \n",
    "    \n",
    "##### Step 2: Update $\\mathbf{H}$ by gradient descent\n",
    "The Euclidean distance $ ||\\mathbf{X} - \\mathbf{W H}||$ is non increasing under the update rules. The Euclidean distance is invariant under these updates if and only if $\\mathbf{W}$ and $\\mathbf{H}$ are at a stationary point of the distance. Hence, we have\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "    \\mathbf{H}_{j}^{(\\tau+1)} = \\mathbf{H}_{j}^{(\\tau)} + \\eta [(\\mathbf{W}^T \\mathbf{X})_{j} - (\\mathbf{W}^T \\mathbf{WH})_{j}]\n",
    "\\end{split}. \\tag{B.2}\n",
    "\\end{equation} \n",
    "where <br>\n",
    "$\\eta = \\frac{\\mathbf{H}}{\\mathbf{W}^T\\mathbf{WH}}$: learning rate <br> \n",
    "$\\tau$: the iteration number.<br>\n",
    "$j$: the sample number $j \\in \\boldsymbol{N}^{+}$ and $\\leq n$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 3: Update the coefficient matrix $\\mathbf{W}$\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "    \\mathbf{W}_{j}^{(\\tau+1)} =  \\mathbf{W}_{j}^{(\\tau)} + \\eta [(\\mathbf{X} \\mathbf{H}^T)_{j} - (\\mathbf{WH} \\mathbf{H}^T)_{j}]\n",
    "\\end{split}. \\tag{B.3}\n",
    "\\end{equation} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 4: repeat until \n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "    \\lVert \\sum_{i=1}^{n} \\mathbf{X}_i - (\\mathbf{WH})_i  \\lVert \\approx 0 \n",
    "\\end{split}. \\tag{B.4}\n",
    "\\end{equation} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective function of sklearn.decomposition.NMF is:\n",
    "    \n",
    "\\begin{align}\n",
    "\\begin{aligned}\n",
    "L(\\mathbf{W}, \\mathbf{H}) &= 0.5 * ||\\mathbf{X} - \\mathbf{WH}||_{loss}^2\\\\&+ \\alpha_W * l1_{ratio} * n_{features} * ||vec(\\mathbf{W})||_1\\\\&+ \\alpha_H * l1_{ratio} * n_{samples} * ||vec(\\mathbf{H})||_1\\\\&+ 0.5 * \\alpha_W * (1 - l1_{ratio}) * n_{features} * ||\\mathbf{W}||_{Fro}^2\\\\&+ 0.5 * \\alpha_H * (1 - l1_{ratio}) * n_{samples} * ||\\mathbf{H}||_{Fro}^2\n",
    "\\end{aligned}. \\tag{B.5}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note: \n",
    "- PCA: generate a face from multiple reduced faces\n",
    "- NMF: synthesis a face based on key basis (features: such as eye from A, node from B,...), in other words, $\\mathbf{H}$ is a feature matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B.1 Applying NMF to synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "nmf = NMF(n_components=15, random_state=0, max_iter=20000)\n",
    "nmf.fit(people_X_train)\n",
    "people_X_train_nmf = nmf.transform(people_X_train)\n",
    "people_X_test_nmf = nmf.transform(people_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 5, figsize=(8, 5),\n",
    "                         subplot_kw={'xticks': (), 'yticks': ()})\n",
    "for i, (component, ax) in enumerate(zip(nmf.components_, axes.ravel())):\n",
    "    ax.imshow(component.reshape(image_shape))\n",
    "    ax.set_title(\"Comp. #{}\".format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compn = 3\n",
    "# sort by 3rd component, plot first 10 images\n",
    "inds = np.argsort(people_X_test_nmf[:, compn])[::-1]\n",
    "fig, axes = plt.subplots(2, 5, figsize=(8, 5),\n",
    "                         subplot_kw={'xticks': (), 'yticks': ()})\n",
    "fig.suptitle(\"Large component 3\")\n",
    "for i, (ind, ax) in enumerate(zip(inds, axes.ravel())):\n",
    "    ax.imshow(people_X_test[ind].reshape(image_shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/@zahrahafida.benslimane/audio-source-separation-using-non-negative-matrix-factorization-nmf-a8b204490c7d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid green\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. t-SNE (t-Distributed Stochastic Neighbor Embedding)\n",
    "https://builtin.com/data-science/tsne-python <br>\n",
    "Understanding t-SNE: https://newmortalbeing.medium.com/understanding-t-sne-d869ece441a5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid grey\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive_Bayes_Implementation: \n",
    "http://localhost:8888/notebooks/Machine%20Learning%20with%20Application/3.%20Unsupervised%20Learning/Naive_Bayes_Implementation.ipynb#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid grey\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* t-SNE is an unsupervised, non-parametric method for dimensionality reduction developed by Laurens van der Maaten and Geoffrey Hinton in 2008.\n",
    "* ‘Non-parametric’ because it doesn’t construct an explicit function that maps high dimensional points to a low dimensional space. \n",
    "* Instead, it just optimizes low dimensional positions of the data points directly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- t-SNE 主要是將高維的數據用高斯分佈的機率密度函數近似，而低維數據的部分使用 t 分佈的方式來近似，https://www.jmp.com/zh_tw/statistics-knowledge-portal/t-test/t-distribution.html\n",
    "- 使用 KL 距離計算相似度，https://machinelearningmastery.com/divergence-between-probability-distributions/\n",
    "- 最後再以梯度下降（或隨機梯度下降）求最佳解 。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1. Define similarities between points<br> \n",
    "\n",
    "A Multivariate Gaussian is centered at each point $x_i$ of the dataset. For every other point $x_j$ of the dataset, we calculate the similarity between $x_i$ and $x_j$ as the conditional probability $p_{j|i}$.\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "{p_ {j \\mid i} = \\frac{\\exp(- \\mid  \\mid  x_i -x_j  \\mid  \\mid  ^2 / (2 \\sigma^2_i ))} {\\sum_{k \\neq i} \\exp(- \\mid  \\mid  x_i - x_k  \\mid  \\mid  ^2 / (2 \\sigma^2_i))}}\n",
    "\\end{split}. \\tag{C.1}\n",
    "\\end{equation}\n",
    "\n",
    "where $\\sigma_i$ is the variance of Gaussian distributed with mean $x_i$, and $p_{x∣x} =0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:640/format:webp/1*fYhwN3wKr3NwDSJFePqwqQ.png\" width=\"300\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This probability is proportional to the probability density of the Gaussian kernel centered at $x_i$. \n",
    "So, for any $x_j$( for example, the yellow data point) that is close to $x_i$, this probability would be high and for any far away $x_j$ (the red point), it is going to be very small but never zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2. Low dimension similarities\n",
    "* SNE arranges all the data points on the required low dimensional space by randomly sampling from a Gaussian that has a very small variance and is centered around the origin. \n",
    "* Note that we are not trying to preserve distances here since it is impossible to preserve all high dimensional distances between points in the low dimensional space. \n",
    "* Instead, we try to preserve the order/rank of distances between points in SNE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The similarity between points in the low dimensional space is given by the conditional probability qⱼ|ᵢ .\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "{q_ {j \\mid i} = \\frac{\\exp(- \\mid  \\mid  y_i -y_j  \\mid  \\mid  ^2 )} {\\sum_{k \\neq i} \\exp(- \\mid  \\mid  y_i - y_k  \\mid  \\mid  ^2 )}}\n",
    "\\end{split}. \\tag{C.2}\n",
    "\\end{equation}\n",
    "\n",
    "It calculates the similarities for all pair of points to produce the embedding similarity matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3. Loss function and optimization\n",
    "\n",
    "* We want the low dimensional mappings {$y_1$, $y_2$,…., $y_n$} to have similar probability distributions as their high dimensional counterparts so as to preserve the relationship between the points in the embedding.\n",
    "\n",
    "* For this, the sum of KL divergences between the pairwise similarities in the high dimension and low dimensional spaces is used as the loss function. \n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "J = \\sum_i \\text{KL}(P_i \\Vert Q_i) = \\sum_i\\sum_j p_{j \\vert i} \\log \\frac{p_{j \\vert i}}{q_{j \\vert i}}\n",
    "\\end{split}. \\tag{C.3}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* KL Divergence function is asymmetric by nature, meaning KL($P_i$ || $Q_i$) ≠ KL( $Q_i$ || $P_i$).<br>\n",
    "And this property of KL Divergence is leveraged by the algorithm to focus on preserving the local structure of the data. \n",
    "\n",
    "* If $p_{j \\vert i}$ is large and $q_{j \\vert i}$  is small, there is a large cost/penalty for representing nearby points in the high dimensional space by widely separated points in the low dimensional map. This explains why the technique is good at preserving the local structure.\n",
    "* However, if$p_{j \\vert i}$  is small, any $q_{j \\vert i}$  will have almost similar small penalties.\n",
    "* In other words, far away points in high dimensional space can be be placed just anywhere on the low dimensional map."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After initializing the low dimensional mappings and calculating the similarities, the loss function is optimized using Gradient Descent to arrive at the final embedding.\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\frac{\\partial J}{\\partial y_i} = 2 \\sum_{j \\neq i} (p_{j \\vert i} - q_{j \\vert i} + p_{i \\vert j} - q_{i \\vert j})(y_i - y_j)\n",
    "\\end{split}. \\tag{C.4}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Update in each iteration :\n",
    "    \n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "y^{(k)} = y^{(k-1)} + \\eta \\frac{\\partial J}{\\partial y_i} + \\alpha(k) (y^{(k-1)}-y^{(k-2)})\n",
    "\\end{split}. \\tag{C.5}\n",
    "\\end{equation}\n",
    "                                                                                       \n",
    "where $\\alpha(k)$ is monentum at iteration k, $y^{(k)}$ is the soluation at iteration k, and $\\eta$ is learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "tsne = TSNE(random_state=42)\n",
    "# use fit_transform instead of fit, as TSNE has no transform method\n",
    "digits_tsne = tsne.fit_transform(digits.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 5))\n",
    "plt.xlim(digits_tsne[:, 0].min(), digits_tsne[:, 0].max() + 1)\n",
    "plt.ylim(digits_tsne[:, 1].min(), digits_tsne[:, 1].max() + 1)\n",
    "for i in range(len(digits.data)):\n",
    "    # actually plot the digits as text instead of using scatter\n",
    "    plt.text(digits_tsne[i, 0], digits_tsne[i, 1], str(digits.target[i]),\n",
    "             color = colors[digits.target[i]],\n",
    "             fontdict={'weight': 'bold', 'size': 9})\n",
    "plt.xlabel(\"t-SNE feature 0\")\n",
    "plt.ylabel(\"t-SNE feature 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Problems with SNE and the modifications made\n",
    "* Cost function of SNE is difficult to optimize\n",
    "* Crowding Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* One good practice is to first use PCA to reduce the number of dimensions (if d is large) and then use t-SNE.\n",
    "\n",
    "* Many ways have been suggested over the years to speed up the computation of the attractive and repulsive forces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### D. 均勻流形逼近及投影(Uniform Manifold Approximation and Projection, UMAP)\n",
    "UMAP is a manifold learning technique that aims to reduce the dimensionality of data while preserving its topological structure. <br>\n",
    "It is particularly useful for visualizing high-dimensional datasets in a low-dimensional space, typically two or three dimensions. <br>\n",
    "UMAP is often compared to t-SNE (t-distributed Stochastic Neighbor Embedding) due to its similar application in data visualization, but it offers several advantages, including better preservation of global data structure and faster computation times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://umap-learn.readthedocs.io/en/latest/basic_usage.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.geeksforgeeks.org/umap-uniform-manifold-approximation-and-projection/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install umap-learn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import umap\n",
    "\n",
    "scaler = StandardScaler()\n",
    "digits_umap = scaler.fit_transform(digits.data)\n",
    "\n",
    "# Apply UMAP\n",
    "reducer = umap.UMAP()\n",
    "embedding = reducer.fit_transform(digits_umap)\n",
    "plt.scatter(embedding[:, 0], embedding[:, 1], c=digits.target, cmap='Spectral', s=5)\n",
    "plt.colorbar(boundaries=np.arange(11)-0.5).set_ticks(np.arange(10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of Clustering Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ref\n",
    "* Naive Bayes, Clearly Explained: https://www.youtube.com/watch?v=O2L2Uv9pdDA \n",
    "* Gaussian Naive Bayes, Clearly Explained: https://www.youtube.com/watch?v=H3EjCKtlVog \n",
    "* A Step-by-Step Explanation of Principal Component Analysis (PCA): https://builtin.com/data-science/step-step-explanation-principal-component-analysis \n",
    "* t-SNE, Clearly Explained: https://www.youtube.com/watch?v=NEaUSP4YerM        \n",
    "* Naive Bayes Classifier: https://towardsdatascience.com/naive-bayes-classifier-81d512f50a7c"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
