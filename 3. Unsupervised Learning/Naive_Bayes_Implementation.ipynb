{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tNGBMrKizCMt"
   },
   "source": [
    "# Naïve Bayes Implementation\n",
    "\n",
    "### 貝氏分類器Naive Bayes Classifier \n",
    "https://ithelp.ithome.com.tw/articles/10297660"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T6BSWTAVJPvh"
   },
   "source": [
    "There is a feature set $\\mathbf{x}=\\{x_1, x_2,...,x_p\\}$, which can be divided in class $y_k$ in classes $C={y_1,...,y_K}$. \n",
    "\\begin{equation}\\begin{split}\n",
    "p(y_k|\\mathbf{x}) &= \\frac{p(\\mathbf{x}|y_k)p(y_k)}{p(\\mathbf{x})}\\\\\n",
    "\\text{posterior} &= \\frac{\\text{prior} \\times \\text{likehood}}{\\text{evdience}}\n",
    "\\end{split} \\tag{1}\n",
    "\\end{equation}\n",
    "where <br>\n",
    "$\\ p(y_k|\\mathbf{x})$ is the posterior probability of class ($y_{k}$, target) given predictor ($\\mathbf{x}$). <br>\n",
    "$\\ p(y_k)$ is the prior probability of class.<br>\n",
    "$\\ p(\\mathbf{x}|y_k)$ is the likelihood which is the probability of the predictor given class.<br>\n",
    "$\\ p(\\mathbf{x})$ is the prior probability of the predictor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T6BSWTAVJPvh"
   },
   "source": [
    "\\begin{equation}\\begin{split}\n",
    "p(y_k, x_1,...,x_p) &= p(x_1,...,x_p, y_k)\\\\\n",
    "                    &= p(x_1 | x_2,...,x_p, y_k)p(x_2,...,x_p, y_k)\\\\\n",
    "                    &= p(x_1 | x_2,...,x_p, y_k)p(x_2 |x_3,...,x_p, y_k)p(x_3,...,x_p, y_k)\\\\\n",
    "                    &= ....\\\\\n",
    "                    &= p(x_1 | x_2,...,x_p, y_k)p(x_2 |x_3,...,x_p, y_k)...p(x_p | y_k)p(y_p)\n",
    "\\end{split} \\tag{2}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the \"naive\" conditional independence assumes that all features in $\\mathbf{x}$  are <span style=\"color:red\">mutually independent</span>, conditional on the category $y_k$.<br>\n",
    "Under this assumption,\n",
    "\\begin{equation}\\begin{split}\n",
    "p(x_i | x_{i+1},...,x_p,y_k) = p(x_i| y_k)\n",
    "\\end{split}. \\tag{3}\n",
    "\\end{equation}\n",
    "Thus, the joint model can be expressed as\n",
    "\\begin{equation}\\begin{split}\n",
    "p(y_k | x_1,...,x_p)  \\varpropto p(y_k, x_1,...,x_p) &= p(y_k) p(x_1| y_k) p(x_2| y_k)... \\\\\n",
    "                &= p(y_k) \\prod_{i=1}^{p} p(x_i| y_k)\n",
    "\\end{split}  \\tag{4}\n",
    "\\end{equation}\n",
    "where $\\varpropto$ denotes proportionality since we omited the denominator $p(x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that under the above independence assumptions, the conditional distribution over the class variable $y$ is:\n",
    "\\begin{equation}\\begin{split}\n",
    "p(y_k | x_1,...,x_p) = \\frac{1}{Z}\\prod_{i=1}^{p} p(x_i| y_k)\n",
    "\\end{split} . \\tag{5}\n",
    "\\end{equation}\n",
    "where the evidence $Z=p(\\mathbf {x} )=\\sum _{k}p(y_{k})\\ p(\\mathbf {x} \\mid y_{k})$ is a scaling factor dependent only on $x_{1},\\ldots ,x_{n}$. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is, a constant if the values of the feature variables are known.<br>\n",
    "If we use Maximum A Posteriori (MAP) estimation to estimate $p(y_k)$ and $p(x_i|y_k)$; the former is then the relative frequency of class $y_k$\n",
    "in the training set.\n",
    "\\begin{equation}\\begin{split}\n",
    "\\hat{y}_k = \\arg \\max_{y_k} p(y_k) \\prod_{i=1}^{p} p(x_i| y_k)\n",
    "\\end{split} . \\tag{6}\n",
    "\\end{equation}\n",
    "The different naive Bayes classifiers differ mainly by the assumptions they make regarding the distribution of $p(x_i|y_k)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There isn’t just one type of Naïve Bayes classifier. The most popular types differ based on the distributions of the feature values. Some of these include: \n",
    "\n",
    "- Gaussian Naïve Bayes (GaussianNB): This is a variant of the Naïve Bayes classifier, which is used with Gaussian distributions—i.e. normal distributions—and continuous variables. This model is fitted by finding the mean and standard deviation of each class. \n",
    "- Multinomial Naïve Bayes (MultinomialNB): This type of Naïve Bayes classifier assumes that the features are from multinomial distributions. This variant is useful when using discrete data, such as frequency counts, and it is typically applied within natural language processing use cases, like spam classification. \n",
    "- Bernoulli Naïve Bayes (BernoulliNB): This is another variant of the Naïve Bayes classifier, which is used with Boolean variables—that is, variables with two values, such as True and False or 1 and 0. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Naive Bayes\n",
    "GaussianNB implements the Gaussian Naive Bayes algorithm for classification. The likelihood of the features is assumed to be Gaussian:\n",
    "\\begin{equation}\\begin{split}\n",
    "p(x_i | y) = p(x_i| \\mu_{y}, \\sigma_{y}) = \\frac{1}{\\sqrt{2 \\pi \\sigma_{y}^{2}}} \\exp (-\\frac{(x_i- \\mu_y)^2}{2 \\sigma_{y}^2})\n",
    "\\end{split} . \\tag{7}\n",
    "\\end{equation}\n",
    "where the parameters $\\sigma_{y}$ and $\\mu_{y}$ are estimated using maximum likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T6BSWTAVJPvh"
   },
   "source": [
    "For, let $y = y_k$. Now, the question is how to maximize the likelihood estimate of the normal distribution parameters. <br>\n",
    "To find the maximum likelihood estimation (MLE) of the two parameters. <br>\n",
    "We can define the likelihood function with $x_1,...,x_p$ as\n",
    "\\begin{equation}\\begin{split}\n",
    "L(\\mu, \\sigma| x_1,...,x_p) &= \\prod_{i=1}^{p} p(x_i|\\mu, \\sigma^2) \\\\\n",
    "                            &= \\prod_{i=1}^{p} \\frac{1}{\\sqrt{2 \\pi \\sigma^{2}}} \\exp (-\\frac{(x_i- \\mu)^2}{2 \\sigma^2}) \\\\\n",
    "                            &= \\frac{1}{({2 \\pi \\sigma^{2}})^{p/2}} \\exp (- \\frac{1}{2 \\sigma^2} \\sum_{i=1}^{p} (x_i- \\mu)^2)\n",
    "\\end{split}  \\tag{8}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The log-likelihood function is \n",
    "\\begin{equation}\\begin{split}\n",
    "l(\\mu, \\sigma| x_1,...,x_p) &= \\ln(L(\\mu, \\sigma; x_1,...,x_p)) \\\\\n",
    "                            &= -\\frac{p}{2} \\ln(2\\pi) - \\frac{p}{2}\\ln{(\\sigma^2)} -\\frac{1}{2\\sigma^2} \\sum_{i=1}^{p}(x_i- \\mu)^2\n",
    "\\end{split} \\tag{9}\n",
    "\\end{equation}\n",
    "\n",
    "To solve the maximization problem\n",
    "\\begin{equation}\\begin{split}\n",
    "\\max_{\\mu,\\sigma^2} l(\\mu, \\sigma| x_1,...,x_p)\n",
    "\\end{split}  \\tag{10}\n",
    "\\end{equation}\n",
    "\n",
    "The first-order conditions for a maximum are\n",
    "\\begin{equation}\\begin{split}\n",
    "\\frac{\\partial}{\\partial \\mu} l(\\mu, \\sigma| x_1,...,x_p) = 0 \\\\\n",
    "\\frac{\\partial}{\\partial \\sigma} l(\\mu, \\sigma| x_1,...,x_p) = 0\n",
    "\\end{split} \\tag{11}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The partial derivative of the log-likelihood for the mean is\n",
    "\\begin{equation}\\begin{split}\n",
    "\\frac{\\partial}{\\partial \\mu} l(\\mu, \\sigma| x_1,...,x_p) \n",
    "&= \\frac{\\partial}{\\partial \\mu} [-\\frac{p}{2} \\ln(2\\pi) - \\frac{p}{2}\\ln{(\\sigma^2)} -\\frac{1}{2\\sigma^2} \\sum_{i=1}^{p}(x_i- \\mu)^2] \\\\\n",
    "&= \\frac{1}{\\sigma^2} \\sum_{i=1}^{p}(x_i- \\mu) \\\\\n",
    "&= \\frac{1}{\\sigma^2} (\\sum_{i=1}^{p}x_i- p\\mu)\n",
    "\\end{split}  \\tag{12}\n",
    "\\end{equation}\n",
    "which is equal to zero only if $(\\sum_{i=1}^{p}x_i- p\\mu) = 0$ <br>\n",
    "Therefore, the first of the two first-order conditions implies\n",
    "\\begin{equation}\\begin{split}\n",
    "\\hat{\\mu} = \\frac{1}{p} \\sum_{i=1}^{p}x_i\n",
    "\\end{split} \\tag{13}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we have\n",
    "\\begin{equation}\\begin{split}\n",
    "\\hat{\\sigma}^2 = \\frac{1}{p} \\sum_{i=1}^{p} (x_i-\\hat{\\mu})^2\n",
    "\\end{split} . \\tag{14}\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, the estimator $\\hat{\\mu }$ is equal to the sample mean and the estimator $\\hat{\\sigma}^2$ is equal to the unadjusted sample variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Naive Bayes tutorial is broken down into 5 parts:\n",
    "\n",
    "- Step 1: Separate By Class.\n",
    "- Step 2: Summarize Dataset.\n",
    "- Step 3: Summarize Data By Class.\n",
    "- Step 4: Gaussian Probability Density Function.\n",
    "- Step 5: Class Probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ArpzyzmHy-ra"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 75 points : 4\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "X, y = load_iris(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = StratifiedKFold(X, y, test_size=0.5, random_state=0)\n",
    "\n",
    "gnb = GaussianNB()\n",
    "y_pred = gnb.fit(X_train, y_train).predict(X_test)\n",
    "print(\"Number of mislabeled points out of a total %d points : %d\"\n",
    "      % (X_test.shape[0], (y_test != y_pred).sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix = [[21  0  0]\n",
      " [ 0 30  0]\n",
      " [ 0  4 20]]\n",
      "accuracy = 0.9466666666666667\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix,accuracy_score\n",
    "print(f'confusion matrix = {confusion_matrix(y_test, y_pred)}')\n",
    "print(f'accuracy = {accuracy_score(y_test,y_pred)}')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Naive_Bayes_Implementation.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
